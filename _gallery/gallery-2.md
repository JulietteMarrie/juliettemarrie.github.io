---
layout: gallery
title: PCA of image features for 30 classes of the CUB Bird dataset. Distilling a large pretrained teacher (top, left) to train a small task-specific student model (top, right) results in a better clustering of the representations compared to simply finetuning the student on the task (bottom, right). Distillation can be improved by using a Mixup-inspired class-agnostic data augmentation based on Stable Diffusion (grey features in teacher plot).
image: /files/tmlr.png
description: My second PhD project (<a href="https://openreview.net/forum?id=oyISaaeHwD">TMLR 2024</a>) delineates good practices for leveraging large pretrained visual models to train smaller models on specific tasks.
---
