---
layout: gallery
title: "On Good Practices for Task-Specific Distillation of Large Pretrained Visual Models"
media:
  - type: image
    src: /files/tmlr.png
    caption: PCA of image features for 30 classes of the CUB Bird dataset. Distilling a large pretrained teacher (top, left) to train a small task-specific student model (top, right) results in a better clustering of the representations compared to simply finetuning the student on the task (bottom, right). Distillation can be improved by using a Mixup-inspired class-agnostic data augmentation based on Stable Diffusion (grey features in teacher plot).
description: My second PhD project (<a href="https://openreview.net/forum?id=oyISaaeHwD">TMLR 2024</a>) delineates good practices for leveraging large pretrained visual models to train smaller models on specific tasks.
---
